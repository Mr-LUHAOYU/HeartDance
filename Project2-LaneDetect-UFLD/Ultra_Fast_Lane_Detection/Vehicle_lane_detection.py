from data.constant import tusimple_row_anchor
import scipy
import cv2
import torch
import numpy as np
from ultralytics import YOLO
from model.model import parsingNet
from torchvision import transforms
from PIL import Image
from torchvision.ops import nms
import os
from ultralytics.utils import LOGGER
import subprocess

LOGGER.setLevel(50)
# åˆå§‹åŒ– GPU è®¾å¤‡
device = torch.device("cpu")

# è½½å…¥ UFLD è½¦é“çº¿æ£€æµ‹æ¨¡å‹
# ä¿®æ”¹è½¦é“çº¿æ¨¡å‹å‚æ•°é…ç½®ä»¥åŒ¹é…Tusimpleé¢„è®­ç»ƒæƒé‡
cls_num_per_lane = 56  # ä¿®æ­£ä¸ºTusimpleæ•°æ®é›†æ­£ç¡®çš„åˆ†å‰²ç‚¹æ•°é‡

# ç¡®ä¿æ¨¡å‹é…ç½®ä¸é¢„è®­ç»ƒæƒé‡åŒ¹é…
ufld_model = parsingNet(pretrained=False, backbone='18', 
                      cls_dim=(cls_num_per_lane + 1, 4, 2),  # æ€»ç»´åº¦ä¸º57x4x2ï¼ˆ56ä¸ªåˆ†å‰²ç‚¹+1å­˜åœ¨æ€§åˆ†ç±»ï¼‰
                      use_aux=False).to(device)

# åªåŠ è½½åŒ¹é…çš„æƒé‡
checkpoint = torch.load("checkpoints/tusimple_18.pth", map_location=device)
state_dict = checkpoint["model"]

# # ç§»é™¤ä¸åŒ¹é…çš„ cls.2 å±‚
# filtered_state_dict = {k: v for k, v in state_dict.items() if "cls.2" not in k}
# ufld_model.load_state_dict(filtered_state_dict, strict=False)

ufld_model.eval()

# YOLOv8 è½¦è¾†æ£€æµ‹æ¨¡å‹
yolo_model = YOLO("yolov8n.pt",verbose=False)

# å›¾åƒé¢„å¤„ç†
img_transforms = transforms.Compose([
    transforms.Resize((288, 800)),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])
def detect_plate(image_path):
    """è°ƒç”¨è½¦ç‰Œè¯†åˆ«è„šæœ¬å¹¶è¿”å›è½¦ç‰Œå·"""
    command = [
        "python", "detect_plate.py",
        "--detect_model", "../Chinese_license_plate_detection_recognition/weights/plate_detect.pt",
        "--rec_model", "../Chinese_license_plate_detection_recognition/weights/plate_rec_color.pth",
        "--image_path", image_path,
        "--output", "../image"
    ]
    result = subprocess.run(command, capture_output=True, text=True)
    print("è½¦ç‰Œè¯†åˆ«ç»“æœ:", result.stdout)
    return result.stdout.strip()
def detect_lanes(frame):
    """æ£€æµ‹è½¦é“çº¿å¹¶è½¬æ¢ä¸ºåƒç´ åæ ‡"""
    img_w, img_h = frame.shape[1], frame.shape[0]
    img = cv2.resize(frame, (800, 288))  
    img = Image.fromarray(img)  # è½¬æ¢ä¸º PIL.Image
    img = img_transforms(img).unsqueeze(0).to(device)  

    with torch.no_grad():
        output = ufld_model(img)
    
    col_sample = np.linspace(0, 800 - 1, cls_num_per_lane)  # é‡‡æ ·åˆ—æ•°
    col_sample_w = col_sample[1] - col_sample[0]
    row_anchor = tusimple_row_anchor  # é¢„å®šä¹‰çš„è¡Œé”šç‚¹
    
    out_j = output[0].cpu().numpy()
    out_j = out_j[:, ::-1, :]  # ç¿»è½¬åˆ—ç´¢å¼•
    prob = scipy.special.softmax(out_j[:-1, :, :], axis=0)  # è®¡ç®—softmaxæ¦‚ç‡
    idx = np.arange(cls_num_per_lane) + 1  # è®¡ç®—ç´¢å¼•
    idx = idx.reshape(-1, 1, 1)
    loc = np.sum(prob * idx, axis=0)  # è®¡ç®—æœŸæœ›åˆ—ä½ç½®
    
    out_j = np.argmax(out_j, axis=0)  # è·å–æœ€å¤§æ¦‚ç‡ç´¢å¼•
    loc[out_j == cls_num_per_lane] = 0  # è¿‡æ»¤æ— æ•ˆç‚¹
    out_j = loc  
    
    processed_lanes = []
    for i in range(out_j.shape[1]):  # éå†è½¦é“çº¿
        lane = []
        if np.sum(out_j[:, i] != 0) > 2:  # è¿‡æ»¤æ— æ•ˆè½¦é“
            for k in range(out_j.shape[0]):  # éå†è¡Œåæ ‡
                if out_j[k, i] > 0:
                    x = int(out_j[k, i] * col_sample_w * img_w / 800) - 1
                    y = int(img_h * (row_anchor[cls_num_per_lane - 1 - k] / 288)) - 1
                    lane.append((x, y))
        processed_lanes.append(lane)
    
    return processed_lanes

def detect_vehicles(frame):
    """æ£€æµ‹è½¦è¾†"""
    results = yolo_model(frame)
    if len(results) == 0 or results[0].boxes is None:
        print("æœªæ£€æµ‹åˆ°è½¦è¾†ï¼")
        return np.array([])
    boxes = results[0].boxes.xyxy.cpu().numpy()  # è·å–è½¦è¾†æ£€æµ‹æ¡†
    scores = results[0].boxes.conf.cpu().numpy()  # è·å–ç½®ä¿¡åº¦åˆ†æ•°

    # è½¬æ¢ä¸º PyTorch Tensor
    boxes = torch.tensor(boxes, dtype=torch.float32)
    scores = torch.tensor(scores, dtype=torch.float32)

    # è¿è¡Œ NMS
    keep = nms(boxes, scores, iou_threshold=0.5)

    return boxes[keep].cpu().numpy()  # ç¡®ä¿è¿”å› NumPy æ•°ç»„

def is_vehicle_crossing(vehicle_box, lane_lines):
    """åˆ¤æ–­è½¦è¾†æ˜¯å¦å‹çº¿"""
    vx1, vy1, vx2, vy2 = vehicle_box
    cx, cy = (vx1 + vx2) // 2, vy2  # è½¦è¾†åº•éƒ¨ä¸­å¿ƒç‚¹

    for lane in lane_lines:
        lane_xs = [x for x, y in lane if abs(y - cy) < 430]  # å– y æ¥è¿‘ cy çš„ x å€¼
        if any(vx1 <= x <= vx2 for x in lane_xs):  # è½¦é“çº¿ä¸è½¦è¾†æœ‰äº¤å‰
            print(f"ğŸš— è½¦è¾†å‹çº¿: è½¦ä¸­å¿ƒ {cx, cy}, è½¦é“çº¿ x: {lane_xs}")
            return True
    return False

def adaptive_affine_transform(src_img, dst_img):
    # æ›¿æ¢ä¸ºORBç‰¹å¾æ£€æµ‹
    orb = cv2.ORB_create(nfeatures=1000)
    kp1, des1 = orb.detectAndCompute(src_img, None)
    kp2, des2 = orb.detectAndCompute(dst_img, None)

    # ä½¿ç”¨BruteForce HammingåŒ¹é…å™¨
    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
    matches = bf.match(des1, des2)

    # è¿‡æ»¤åŒ¹é…ç‚¹
    matches = sorted(matches, key=lambda x: x.distance)
    good_matches = matches[:int(len(matches)*0.9)]  # å–å‰90%çš„ä¼˜è´¨åŒ¹é…

    if len(good_matches) < 4:
        return None, None

    # æå–åŒ¹é…ç‚¹åæ ‡
    src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1,1,2)
    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1,1,2)

    # å¢åŠ RANSACè¿­ä»£æ¬¡æ•°å’Œé˜ˆå€¼è°ƒæ•´
    affine_matrix, mask = cv2.estimateAffine2D(src_pts, dst_pts, 
                                              method=cv2.RANSAC, 
                                              ransacReprojThreshold=5.0,
                                              maxIters=2000)

    if affine_matrix is None or np.linalg.det(affine_matrix[:2,:2]) < 1e-6:
        return None, None

    # ä½¿ç”¨åŒçº¿æ€§æ’å€¼é˜²æ­¢å˜å½¢å¤±çœŸ
    transformed_img = cv2.warpAffine(src_img, affine_matrix, 
                                    (dst_img.shape[1], dst_img.shape[0]),
                                    flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)
    return transformed_img, affine_matrix

def bird_view_transform(frame, reference_frame):
    """ç”Ÿæˆä¿¯è§†å›¾ï¼ˆé¸Ÿç°å›¾ï¼‰"""
    transformed_img, _ = adaptive_affine_transform(frame, reference_frame)
    if transformed_img is None:
        print("ä»¿å°„å˜æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸå›¾")
        return frame
    return transformed_img

def draw_annotations(img, lane_lines, boxes, is_top_view=False):
    """åœ¨å›¾åƒï¼ˆåŸå§‹å¸§æˆ–é¸Ÿç°å›¾ï¼‰ä¸Šæ ‡æ³¨è½¦é“çº¿å’Œè½¦è¾†æ¡†"""
    if img is None:
        return None
    img = img.copy()

    # ç”»è½¦é“çº¿
    for lane in lane_lines:
        for (x, y) in lane:
            if x > 0:
                cv2.circle(img, (int(x), int(y)), 5, (0, 255, 0), -1)  # ç»¿è‰²è½¦é“çº¿

    # ç”»è½¦è¾†æ¡†
    for box in boxes:
        x1, y1, x2, y2 = map(int, box)
        color = (0, 0, 255) if is_vehicle_crossing(box, lane_lines) else (255, 0, 0)
        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)

    return img

def process_video(video_path):
    cap = cv2.VideoCapture(video_path)
    ret, ref_frame = cap.read()
    if not ret:
        print("æ— æ³•è¯»å–è§†é¢‘")
        return
    if not os.path.exists("image"):
        os.makedirs("image")

    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        # ç”Ÿæˆé¸Ÿç°å›¾
        bird_view = bird_view_transform(frame, ref_frame)
        
        # æ‰“å°ç”Ÿæˆçš„é¸Ÿç°å›¾
        # cv2.imshow('Bird View', bird_view)
        # cv2.waitKey(3000)
        
        # æ£€æµ‹è½¦é“çº¿ä¸è½¦è¾†æ¡†
        lane_lines = detect_lanes(bird_view)
        # print(lane_lines)
        vehicle_boxes = detect_vehicles(bird_view)

        # æ‰“å°æ ‡æ³¨å¥½çš„é¸Ÿç°å›¾
        annotated_bird = draw_annotations(bird_view, lane_lines, vehicle_boxes)
        # cv2.imshow('Annotated Bird View', annotated_bird)
        # cv2.waitKey(3000)
        
        # æ£€æµ‹æ˜¯å¦å‹çº¿
        crossing_detected = False
        for box in vehicle_boxes:
            if is_vehicle_crossing(box, lane_lines):
                crossing_detected = True
                break
        if crossing_detected:
            print('è¿›å…¥')
            frame_count += 1
            timestamp = int(cap.get(cv2.CAP_PROP_POS_MSEC))
            
            # ç”Ÿæˆæ ‡æ³¨å›¾åƒ
            annotated_bird = draw_annotations(bird_view, lane_lines, vehicle_boxes)
            filename = os.path.splitext(os.path.basename(video_path))[0]
            # ä¿å­˜å›¾åƒ
            cv2.imwrite(f'image/{video_path}_original.jpg', filename)
            cv2.imwrite(f'image/{video_path}_bird.jpg', filename)
            path = f'../image/{filename}_orignal.jpg'
            print(f"ä¿å­˜å‹çº¿å¸§ï¼š{video_path}",filename)
            detect_plate(path)
        # å¤„ç†å®Œç¬¬ä¸€å¸§åé€€å‡ºå¾ªç¯
        if frame_count == 0:
            break
    cap.release()
    cv2.destroyAllWindows()

# è¿è¡Œ
process_video("../è½¦å‹çº¿è§†é¢‘/32010120210610092038370.mp4")